{"cells":[{"cell_type":"code","source":["import pyspark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["print('Hello World')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["x = 10\ny = 5\nx + y"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["pip install koalas"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import databricks.koalas as ks"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import pandas as pd\npdf = pd.DataFrame({'x':range(3), 'y':['a','b','b'], 'z':['a','b','b']})\n\n# Create a Koalas DataFrame from pandas DataFrame\ndf = ks.from_pandas(pdf)\n\n# Rename the columns\ndf.columns = ['x', 'y', 'z1']\n\n# Do some operations in place:\ndf['x2'] = df.x * df.x"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.describe()\ndf.plot.line()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["file_loc = \"/FileStore/tables/accepted_2007_to_2018Q4-1.csv\"\nfile_type = \"csv\"\n\ninfer_schema = \"true\"\nfirst_row = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type)  \\\n     .option(\"inferSchema\", infer_schema)  \\\n     .option(\"header\", first_row)  \\\n     .option(\"sep\", delimiter)  \\\n     .load(file_loc)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["temp = \"stuff\"\ndf.createOrReplaceTempView(temp)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sql\n\nselect * from stuff"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\nselect distinct Gender from stuff"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df.stat.cov('CoapplicantIncome', 'LoanAmount')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["file_loc = \"/FileStore/tables/accepted_2007_to_2018Q4-1.csv\"\nfile_type = \"csv\"\n\ninfer_schema = \"true\"\nfirst_row = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type)  \\\n     .option(\"inferSchema\", infer_schema)  \\\n     .option(\"header\", first_row)  \\\n     .option(\"sep\", delimiter)  \\\n     .load(file_loc)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["temp = \"stuff\"\ndf.createOrReplaceTempView(temp)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\n\nselect * from stuff"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["stuff_df = spark.table('stuff')\ndisplay(stuff_df)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(stuff_df.describe())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\n\nselect funded_amnt, grade from stuff group by grade, funded_amnt"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import isnan,when,count,col,log\ndisplay(stuff_df.groupBy(\"emp_title\").agg((count(col(\"loan_amnt\"))).alias(\"count\")))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dbutils.fs.unmount(\"/mnt/s3data\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["import urllib\nAWS_K1 = \"AKIA4KXK4VQUOFOE35TX\"\nAWS_S1 = \"/Kqs4ANA4QdYjzbfSD5u+dEx9oVr2t5nVN74JG/9\"\nENCODED_SECRET_KEY = urllib.parse.quote(AWS_S1, \"\")\nAWS_BUCKET_NAME = \"sai-spark9\"\nMOUNT_NAME = \"s3data\"\ndbutils.fs.mount(\"s3n://%s:%s@%s\" % (AWS_K1, ENCODED_SECRET_KEY,AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/s3data\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["file_loc = \"dbfs:/mnt/s3data/listings.csv\"\nfile_type = \"csv\"\n\ninfer_schema = \"true\"\nfirst_row = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type)  \\\n     .option(\"inferSchema\", infer_schema)  \\\n     .option(\"header\", first_row)  \\\n     .option(\"sep\", delimiter)  \\\n     .load(file_loc)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(df.schema)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\ndf_listing=df.withColumn(\"scrape_id\",col(\"scrape_id\").cast(DoubleType()))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# File location and type\nfile_location = \"dbfs:/mnt/s3data/neighbourhoods.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\nneighbourhoodSchema = StructType([\n    StructField(\"neighbourhood_group\", StringType(), True),        \n    StructField(\"neighbourhood\", StringType(), True)\n])\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_neighbourhoods = spark.read.format(file_type) \\\n  .option(\"infer_schema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_neighbourhoods)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# File location and type\nfile_location = \"dbfs:/mnt/s3data/reviews.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_reviews = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_reviews)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\n\ndrop table listing"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\ntb_name = \"listing\"\ndf.write.format(\"parquet\").saveAsTable(tb_name)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%sql\n\ndrop table neibhour"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\ntb_name = \"neibhour\"\ndf_neighbourhoods.write.format(\"parquet\").saveAsTable(tb_name)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%sql\n\ndrop table review"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\ntb_name = \"review\"\ndf_reviews.write.format(\"parquet\").saveAsTable(tb_name)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%sql\nselect * from listing"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(df.groupBy('neighbourhood').count())"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df.groupBy('neighbourhood').count().orderBy(col('count').desc()).show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["df.groupBy('neighbourhood', 'price').agg(({\"price\": \"average\"})).show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%sql\nselect distinct price from listing"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\ndef trunc_stuff(string):\n  return string.strip('$')\n\nspark.udf.register(\"trunc_stuff\",trunc_stuff)\ntrim_func_udf = udf(trunc_stuff)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\ndf_neigh = df.groupBy(\"neighbourhood\").agg(({\"neighbourhood\": \"count\"})).withColumnRenamed(\"count(neighbourhood)\", \"count\")\ndf_neigh = df_neigh.withColumn('percentage',f.round((f.col('count')/f.sum('count').over(Window.partitionBy())*100),3))\ndisplay(df_neigh)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(df_neigh.orderBy('percentage', ascending=False))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["n_grp=df_neigh.alias(\"nc\").join(df_neighbourhoods.alias(\"ne\")).select(\"nc.*\",\"ne.neighbourhood_group\")\ndisplay(n_grp)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["display(n_grp.orderBy('percentage', ascending=False))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["df_reviews.groupBy(\"listing_id\").count().explain()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["x= df.alias(\"list\").join(df_reviews.alias(\"rev\")).select(\"list.id\").groupBy(\"list.id\").count()\ndisplay(x.orderBy('count', ascending=False))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["df_reviews.groupBy(\"listing_id\").count().filter((\"count\") > 30 ).orderBy('count', ascending=False).show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["display(df_reviews.describe())"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%sql\n\nselect distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \nfrom (select listing_id, neighbourhood, price\nfrom listing a left outer join review b on a.id = b.listing_id)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%sql\n\nselect\nlisting_id, neighbourhood, price, review_cnt, dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\nfrom\n(\nselect distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \nfrom (select listing_id, neighbourhood, price\nfrom listing a left outer join review b on a.id = b.listing_id)\n)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["%sql\nselect\nlisting_id, neighbourhood, price, review_cnt,rank from \n(\nselect\nlisting_id, neighbourhood, price, review_cnt, dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\nfrom\n(\nselect distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \nfrom (select listing_id, neighbourhood, price\nfrom listing a left outer join review b on a.id = b.listing_id)\n)) where rank <= 2"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\n\nselect room_type, avg(price) as avg_price, count(*) as count from listing group by room_type"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["fl = \"/FileStore/tables/WA_Fn_UseC__Telco_Customer_Churn.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .option('nanValue', ' ')\\\n  .option('nullValue', ' ')\\\n  .load(fl)\n\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col\ndf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["table_name = \"churns\"\ndf.createOrReplaceTempView(table_name)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["pd_df = df.toPandas"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df.groupBy('Churn').count().show()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["df.select('tenure','TotalCharges','MonthlyCharges').describe().show()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["df.stat.crosstab(\"SeniorCitizen\", \"InternetService\").show()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["df.stat.freqItems([\"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"], 0.6).collect()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["data_prep = df\n(train_data, test_data,) = data_prep.randomSplit([0.7, 0.3], 24)\n\nprint(\"Tot Records Train\" + str(train_data.count()))\nprint(\"Tot Records Test\" + str(test_data.count()))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n#from pyspark.ml.feature.OneHotEncoderEstimator import *\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\ncatCol=[\"gender\",\"SeniorCitizen\",\"Partner\",\"Dependents\",\"tenure\",\"PhoneService\",\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\"PaymentMethod\"]"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nstages = []\n\nfor x in catCol:\n  strIndx = StringIndexer(inputCol=x, outputCol=x + \"Index\")\n  encoder = OneHotEncoder(inputCols=[strIndx.getOutputCol()], outputCols=[x + \"catVec\"])\n  stages += [strIndx, encoder]\n  \nstages"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nimputer = Imputer(inputCols=[\"TotalCharges\"], outputCols=[\"Out_TotalCharges\"])\nstages += [imputer]"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["label_Idx = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\nstages += [label_Idx]"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["df.stat.corr('TotalCharges', 'MonthlyCharges')"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%sql\nselect cast(tenure as int), churn, count(*) as churned from churns where churn='Yes' group by tenure, churn order by cast(tenure as int)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\ntenure_bin = QuantileDiscretizer(numBuckets=3, inputCol=\"tenure\", outputCol=\"tenure_bin\")\nstages += [tenure_bin]"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["numericCols = [\"tenure_bin\", \"Out_TotalCharges\",\"MonthlyCharges\"]\nassembleInputs = assemblerInputs = [c + \"catVec\" for c in catCol] + numericCols\nassembler = VectorAssembler(inputCols=assembleInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["pipeline = Pipeline().setStages(stages)\npipelineModel = pipeline.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["trainprepDF = pipelineModel.transform(train_data)\ntestprepDF = pipelineModel.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainprepDF)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\npredictions = lrModel.transform(testprepDF)\nevaluatorLR = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorLR.evaluate(predictions)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)\n\nevaluatorLR.getMetricName()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nresults = predictions.select(['prediction', 'label'])\n \n## prepare score-label set\nresults_collect = results.collect()\nresults_list = [(float(i[0]), float(i[1])) for i in results_collect]\npredictionAndLabels = sc.parallelize(results_list)\n \nmetrics = BinaryClassificationMetrics(predictionAndLabels)\n\n# Area under precision-recall curve\nprint(\"Area under PR = %s\" % metrics.areaUnderPR)\n\n# Area under ROC curve\nprint(\"Area under ROC = %s\" % metrics.areaUnderROC)\n\npredictions.show(1)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["import pandas as pd\nimport databricks.koalas as ks"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["%fs cp dbfs:/FileStore/tables/accepted_2007_to_2018Q4-1.csv file:/tmp/LaodStats_Q4.csv\n"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["loan_data = ks.read_csv(\"dbfs:/FileStore/tables/accepted_2007_to_2018Q4-1.csv\")\nloan_data.head(5)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["loan_data['loan_status'].value_counts()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["ks.sql(\"select * from {loan_data} where loan_amnt > 20000\")"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":82}],"metadata":{"name":"saispark","notebookId":3257594701891849},"nbformat":4,"nbformat_minor":0}
