{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "name": "saispark",
    "notebookId": 3257594701891849,
    "colab": {
      "name": "saispark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetreks/SaiSpark/blob/master/saispark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM9sWjILhP9q"
      },
      "source": [
        "import pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGE7V-tihP9z"
      },
      "source": [
        "print('Hello World')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x0mxFzEhP95"
      },
      "source": [
        "x = 10\n",
        "y = 5\n",
        "x + y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWl3KT79hP9-"
      },
      "source": [
        "pip install koalas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW4ZzMuhP-F"
      },
      "source": [
        "import databricks.koalas as ks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD9nzesshP-K"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({'x':range(3), 'y':['a','b','b'], 'z':['a','b','b']})\n",
        "\n",
        "# Create a Koalas DataFrame from pandas DataFrame\n",
        "df = ks.from_pandas(pdf)\n",
        "\n",
        "# Rename the columns\n",
        "df.columns = ['x', 'y', 'z1']\n",
        "\n",
        "# Do some operations in place:\n",
        "df['x2'] = df.x * df.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX80DelDhP-P"
      },
      "source": [
        "df.describe()\n",
        "df.plot.line()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBWdjU_jhP-V"
      },
      "source": [
        "file_loc = \"/FileStore/tables/accepted_2007_to_2018Q4-1.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "infer_schema = \"true\"\n",
        "first_row = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "df = spark.read.format(file_type)  \\\n",
        "     .option(\"inferSchema\", infer_schema)  \\\n",
        "     .option(\"header\", first_row)  \\\n",
        "     .option(\"sep\", delimiter)  \\\n",
        "     .load(file_loc)\n",
        "\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inx7n_MjhP-a"
      },
      "source": [
        "temp = \"stuff\"\n",
        "df.createOrReplaceTempView(temp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVQNlaPhP-f"
      },
      "source": [
        "%sql\n",
        "\n",
        "select * from stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D87JA28shP-l"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn19Zj-FhP-r"
      },
      "source": [
        "%sql\n",
        "select distinct Gender from stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q67SM_x2hP-v"
      },
      "source": [
        "df.stat.cov('CoapplicantIncome', 'LoanAmount')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx23_OCOhP-0"
      },
      "source": [
        "file_loc = \"/FileStore/tables/accepted_2007_to_2018Q4-1.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "infer_schema = \"true\"\n",
        "first_row = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "df = spark.read.format(file_type)  \\\n",
        "     .option(\"inferSchema\", infer_schema)  \\\n",
        "     .option(\"header\", first_row)  \\\n",
        "     .option(\"sep\", delimiter)  \\\n",
        "     .load(file_loc)\n",
        "\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXDcDv6hP-5"
      },
      "source": [
        "temp = \"stuff\"\n",
        "df.createOrReplaceTempView(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrSG2gJJhP-9"
      },
      "source": [
        "%sql\n",
        "\n",
        "select * from stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayawUj6shP_C"
      },
      "source": [
        "stuff_df = spark.table('stuff')\n",
        "display(stuff_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWjH1RSNhP_H"
      },
      "source": [
        "display(stuff_df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO3wvxdwhP_M"
      },
      "source": [
        "%sql\n",
        "\n",
        "select funded_amnt, grade from stuff group by grade, funded_amnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UphfbJHOhP_R"
      },
      "source": [
        "from pyspark.sql.functions import isnan,when,count,col,log\n",
        "display(stuff_df.groupBy(\"emp_title\").agg((count(col(\"loan_amnt\"))).alias(\"count\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVe677ghP_V"
      },
      "source": [
        "dbutils.fs.unmount(\"/mnt/s3data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHDEY0ZXhP_a"
      },
      "source": [
        "import urllib\n",
        "AWS_K1 = \"xxx\"\n",
        "AWS_S1 = \"xxx\"\n",
        "ENCODED_SECRET_KEY = urllib.parse.quote(AWS_S1, \"\")\n",
        "AWS_BUCKET_NAME = \"sai-spark9\"\n",
        "MOUNT_NAME = \"s3data\"\n",
        "dbutils.fs.mount(\"s3n://%s:%s@%s\" % (AWS_K1, ENCODED_SECRET_KEY,AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gjiUjohhP_f"
      },
      "source": [
        "display(dbutils.fs.ls(\"/mnt/s3data\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qu-J5INhP_j"
      },
      "source": [
        "file_loc = \"dbfs:/mnt/s3data/listings.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "infer_schema = \"true\"\n",
        "first_row = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "df = spark.read.format(file_type)  \\\n",
        "     .option(\"inferSchema\", infer_schema)  \\\n",
        "     .option(\"header\", first_row)  \\\n",
        "     .option(\"sep\", delimiter)  \\\n",
        "     .load(file_loc)\n",
        "\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pna94lJZhP_o"
      },
      "source": [
        "display(df.schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-5jCp0YhP_t"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "df_listing=df.withColumn(\"scrape_id\",col(\"scrape_id\").cast(DoubleType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThrrM9JXhP_z"
      },
      "source": [
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agHiEN6nhP_4"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# File location and type\n",
        "file_location = \"dbfs:/mnt/s3data/neighbourhoods.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "neighbourhoodSchema = StructType([\n",
        "    StructField(\"neighbourhood_group\", StringType(), True),        \n",
        "    StructField(\"neighbourhood\", StringType(), True)\n",
        "])\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df_neighbourhoods = spark.read.format(file_type) \\\n",
        "  .option(\"infer_schema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df_neighbourhoods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgsmGaKihP_7"
      },
      "source": [
        "# File location and type\n",
        "file_location = \"dbfs:/mnt/s3data/reviews.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df_reviews = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0hhfPFMhQAA"
      },
      "source": [
        "%sql\n",
        "\n",
        "drop table listing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hORnhOQphQAE"
      },
      "source": [
        "spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\n",
        "tb_name = \"listing\"\n",
        "df.write.format(\"parquet\").saveAsTable(tb_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMjbdRmPhQAJ"
      },
      "source": [
        "%sql\n",
        "\n",
        "drop table neibhour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZRqUj0lhQAM"
      },
      "source": [
        "spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\n",
        "tb_name = \"neibhour\"\n",
        "df_neighbourhoods.write.format(\"parquet\").saveAsTable(tb_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgCIr6e9hQAQ"
      },
      "source": [
        "%sql\n",
        "\n",
        "drop table review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjJKtHlyhQAU"
      },
      "source": [
        "spark.conf.set(\"spark.sql.legacy.allowCreateManagedTableUsingNonemptyLocation\", \"true\")\n",
        "tb_name = \"review\"\n",
        "df_reviews.write.format(\"parquet\").saveAsTable(tb_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ridiffbhQAZ"
      },
      "source": [
        "%sql\n",
        "select * from listing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riG2LtbIhQAe"
      },
      "source": [
        "display(df.groupBy('neighbourhood').count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R852RdlkhQAj"
      },
      "source": [
        "df.groupBy('neighbourhood').count().orderBy(col('count').desc()).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdQo40jYhQAn"
      },
      "source": [
        "df.groupBy('neighbourhood', 'price').agg(({\"price\": \"average\"})).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voL2flYqhQAs"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nma07bPGhQAx"
      },
      "source": [
        "%sql\n",
        "select distinct price from listing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNMNZ96DhQA1"
      },
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "def trunc_stuff(string):\n",
        "  return string.strip('$')\n",
        "\n",
        "spark.udf.register(\"trunc_stuff\",trunc_stuff)\n",
        "trim_func_udf = udf(trunc_stuff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6spw5PLhQA8"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.window import Window\n",
        "df_neigh = df.groupBy(\"neighbourhood\").agg(({\"neighbourhood\": \"count\"})).withColumnRenamed(\"count(neighbourhood)\", \"count\")\n",
        "df_neigh = df_neigh.withColumn('percentage',f.round((f.col('count')/f.sum('count').over(Window.partitionBy())*100),3))\n",
        "display(df_neigh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKL1pYtZhQBA"
      },
      "source": [
        "display(df_neigh.orderBy('percentage', ascending=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBTey-E1hQBG"
      },
      "source": [
        "n_grp=df_neigh.alias(\"nc\").join(df_neighbourhoods.alias(\"ne\")).select(\"nc.*\",\"ne.neighbourhood_group\")\n",
        "display(n_grp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwgiKfIshQBL"
      },
      "source": [
        "display(n_grp.orderBy('percentage', ascending=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhIsyb2zhQBR"
      },
      "source": [
        "df_reviews.groupBy(\"listing_id\").count().explain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STYs3LV0hQBW"
      },
      "source": [
        "x= df.alias(\"list\").join(df_reviews.alias(\"rev\")).select(\"list.id\").groupBy(\"list.id\").count()\n",
        "display(x.orderBy('count', ascending=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWC9k7BThQBb"
      },
      "source": [
        "df_reviews.groupBy(\"listing_id\").count().filter((\"count\") > 30 ).orderBy('count', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T5fB2JWhQBh"
      },
      "source": [
        "display(df_reviews.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6QyQ-obhQBm"
      },
      "source": [
        "%sql\n",
        "\n",
        "select distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \n",
        "from (select listing_id, neighbourhood, price\n",
        "from listing a left outer join review b on a.id = b.listing_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t038dlmfhQBr"
      },
      "source": [
        "%sql\n",
        "\n",
        "select\n",
        "listing_id, neighbourhood, price, review_cnt, dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\n",
        "from\n",
        "(\n",
        "select distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \n",
        "from (select listing_id, neighbourhood, price\n",
        "from listing a left outer join review b on a.id = b.listing_id)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkUWX4AShQBz"
      },
      "source": [
        "%sql\n",
        "select\n",
        "listing_id, neighbourhood, price, review_cnt,rank from \n",
        "(\n",
        "select\n",
        "listing_id, neighbourhood, price, review_cnt, dense_rank() OVER (PARTITION BY neighbourhood ORDER BY review_cnt DESC) as rank\n",
        "from\n",
        "(\n",
        "select distinct listing_id, neighbourhood, price, count(listing_id) over (partition by listing_id order by listing_id) as review_cnt \n",
        "from (select listing_id, neighbourhood, price\n",
        "from listing a left outer join review b on a.id = b.listing_id)\n",
        ")) where rank <= 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guPDdrGkhQB3"
      },
      "source": [
        "%sql\n",
        "\n",
        "select room_type, avg(price) as avg_price, count(*) as count from listing group by room_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agRtfPfYhQB7"
      },
      "source": [
        "fl = \"/FileStore/tables/WA_Fn_UseC__Telco_Customer_Churn.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .option('nanValue', ' ')\\\n",
        "  .option('nullValue', ' ')\\\n",
        "  .load(fl)\n",
        "\n",
        "display(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-YwTXUhQCA"
      },
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2yG3Dv9hQCF"
      },
      "source": [
        "table_name = \"churns\"\n",
        "df.createOrReplaceTempView(table_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380Z2N8bhQCL"
      },
      "source": [
        "pd_df = df.toPandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvAm8pxshQCO"
      },
      "source": [
        "df.groupBy('Churn').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgDQL5yohQCT"
      },
      "source": [
        "df.select('tenure','TotalCharges','MonthlyCharges').describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DoYoPbYhQCZ"
      },
      "source": [
        "df.stat.crosstab(\"SeniorCitizen\", \"InternetService\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBpk3PaShQCc"
      },
      "source": [
        "df.stat.freqItems([\"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"], 0.6).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc4NShELhQCg"
      },
      "source": [
        "data_prep = df\n",
        "(train_data, test_data,) = data_prep.randomSplit([0.7, 0.3], 24)\n",
        "\n",
        "print(\"Tot Records Train\" + str(train_data.count()))\n",
        "print(\"Tot Records Test\" + str(test_data.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6R7TS7jhQCj"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "#from pyspark.ml.feature.OneHotEncoderEstimator import *\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "catCol=[\"gender\",\"SeniorCitizen\",\"Partner\",\"Dependents\",\"tenure\",\"PhoneService\",\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\"PaymentMethod\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XsL5ILzhQCn"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "stages = []\n",
        "\n",
        "for x in catCol:\n",
        "  strIndx = StringIndexer(inputCol=x, outputCol=x + \"Index\")\n",
        "  encoder = OneHotEncoder(inputCols=[strIndx.getOutputCol()], outputCols=[x + \"catVec\"])\n",
        "  stages += [strIndx, encoder]\n",
        "  \n",
        "stages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Y8tAXwhQCs"
      },
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "imputer = Imputer(inputCols=[\"TotalCharges\"], outputCols=[\"Out_TotalCharges\"])\n",
        "stages += [imputer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIvSRMx6hQCv"
      },
      "source": [
        "label_Idx = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
        "stages += [label_Idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jETP93fDhQCy"
      },
      "source": [
        "df.stat.corr('TotalCharges', 'MonthlyCharges')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5QEhJUZhQC1"
      },
      "source": [
        "%sql\n",
        "select cast(tenure as int), churn, count(*) as churned from churns where churn='Yes' group by tenure, churn order by cast(tenure as int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qYqydgchQC4"
      },
      "source": [
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "tenure_bin = QuantileDiscretizer(numBuckets=3, inputCol=\"tenure\", outputCol=\"tenure_bin\")\n",
        "stages += [tenure_bin]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MlvpJawhQC_"
      },
      "source": [
        "numericCols = [\"tenure_bin\", \"Out_TotalCharges\",\"MonthlyCharges\"]\n",
        "assembleInputs = assemblerInputs = [c + \"catVec\" for c in catCol] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assembleInputs, outputCol=\"features\")\n",
        "stages += [assembler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJijX70mhQDC"
      },
      "source": [
        "pipeline = Pipeline().setStages(stages)\n",
        "pipelineModel = pipeline.fit(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsXdEB1JhQDG"
      },
      "source": [
        "trainprepDF = pipelineModel.transform(train_data)\n",
        "testprepDF = pipelineModel.transform(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMLxEaphhQDI"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create initial LogisticRegression model\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# Train model with Training Data\n",
        "lrModel = lr.fit(trainprepDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p22vA9ouhQDL"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "predictions = lrModel.transform(testprepDF)\n",
        "evaluatorLR = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "area_under_curve = evaluatorLR.evaluate(predictions)\n",
        "\n",
        "#default evaluation is areaUnderROC\n",
        "print(\"areaUnderROC = %g\" % area_under_curve)\n",
        "\n",
        "evaluatorLR.getMetricName()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_zABHKYhQDO"
      },
      "source": [
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "results = predictions.select(['prediction', 'label'])\n",
        " \n",
        "## prepare score-label set\n",
        "results_collect = results.collect()\n",
        "results_list = [(float(i[0]), float(i[1])) for i in results_collect]\n",
        "predictionAndLabels = sc.parallelize(results_list)\n",
        " \n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
        "\n",
        "# Area under precision-recall curve\n",
        "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
        "\n",
        "# Area under ROC curve\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
        "\n",
        "predictions.show(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INR466cchQDR"
      },
      "source": [
        "import pandas as pd\n",
        "import databricks.koalas as ks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3mXX_A0hQDT"
      },
      "source": [
        "%fs cp dbfs:/FileStore/tables/accepted_2007_to_2018Q4-1.csv file:/tmp/LaodStats_Q4.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-lnSyOxhQDX"
      },
      "source": [
        "loan_data = ks.read_csv(\"dbfs:/FileStore/tables/accepted_2007_to_2018Q4-1.csv\")\n",
        "loan_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yAla_W-hQDa"
      },
      "source": [
        "loan_data['loan_status'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ07FEuUhQDe"
      },
      "source": [
        "ks.sql(\"select * from {loan_data} where loan_amnt > 20000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_H_VZOhQDh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}