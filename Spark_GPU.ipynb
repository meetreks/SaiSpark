{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark@GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeaKb/3ugakjPftU/jpj+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetreks/SaiSpark/blob/master/Spark_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FggvjoRsoQoA"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-3QQPXrpi7c"
      },
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IjeyH5jqjLK"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJkUcpZ-qnty"
      },
      "source": [
        "!wget https://repo1.maven.org/maven2/ai/rapids/cudf/0.9.2/cudf-0.9.2-cuda10-1.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX5d-6zXrEcB"
      },
      "source": [
        "!wget https://repo1.maven.org/maven2/ai/rapids/xgboost4j-spark_2.x/1.0.0-Beta5/xgboost4j-spark_2.x-1.0.0-Beta5.jar\n",
        "!wget https://repo1.maven.org/maven2/ai/rapids/xgboost4j_2.x/1.0.0-Beta5/xgboost4j_2.x-1.0.0-Beta5.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shpC5nUXsB18"
      },
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmRYERAWsG9r"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tr3Vi3XsLbu"
      },
      "source": [
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/cudf-0.9.2-cuda10-1.jar,/content/xgboost4j_2.x-1.0.0-Beta5.jar,/content/xgboost4j-spark_2.x-1.0.0-Beta5.jar pyspark-shell'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irQ2Oq5ItVRQ"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "#sc.conf.set(\"spark.executor.memory\", \"2g\")\n",
        "#sc.conf.set(\"spark.driver.memory\", \"2g\")\n",
        "sc.sparkContext.addPyFile('/content/xgboost4j-spark_2.x-1.0.0-Beta5.jar')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QjmOZjPtzUJ"
      },
      "source": [
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostClassificationModel, XGBoostClassifier\n",
        "from ml.dmlc.xgboost4j.scala.spark.rapids import GpuDataReader\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmB24DQQwz2Y"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "covtyp = fetch_openml(name='covertype', version=4)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHlevJsyw3wl"
      },
      "source": [
        "cov_df = pd.DataFrame(data= np.c_[covtyp['data'], covtyp['target']],\n",
        "                     columns= covtyp['feature_names'] + ['target'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwIwOnmPxPh6",
        "outputId": "0fc65bc4-3fbf-42ec-e562-238a398a1134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cov_df.memory_usage(index=True).sum()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255645408"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRcaN1FexY89"
      },
      "source": [
        "for cols in cov_df.columns:\n",
        "  cov_df[cols] = pd.to_numeric(cov_df[cols])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lh5foGNxpGg"
      },
      "source": [
        "cov_df['target'] = cov_df['target']-1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODkvuMnlxsYI"
      },
      "source": [
        "train_df=cov_df.sample(frac=0.8,random_state=10) \n",
        "test_df=cov_df.drop(train_df.index)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmhXaXakxx61"
      },
      "source": [
        "train_df.to_parquet(fname='covtype_train.parquet',compression='snappy', index=False)\n",
        "test_df.to_parquet(fname='covtype_test.parquet',compression='snappy', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCWsOc_0x_Hv"
      },
      "source": [
        "import pyarrow.parquet as pq"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXUwUFh9yS30"
      },
      "source": [
        "pq.read_table('covtype_train.parquet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZK1xduJyV2l"
      },
      "source": [
        "train_data = GpuDataReader(sc).format('parquet').load('covtype_train.parquet')\n",
        "test_data = GpuDataReader(sc).format('parquet').load('covtype_test.parquet')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJACBTCKycF1"
      },
      "source": [
        "pq_file=pq.read_table('covtype_train.parquet')\n",
        "label=\"target\"\n",
        "features = [ x for x in pq_file.column_names if x != label ]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDfGU6usyySw"
      },
      "source": [
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEvKOFHwzDYy"
      },
      "source": [
        "import time\n",
        "params = { \n",
        "    'eta': 0.1,\n",
        "    'gamma': 0.1,\n",
        "    'missing': 0.0,\n",
        "    'treeMethod': 'gpu_hist',\n",
        "    'maxDepth': 8, \n",
        "    'growPolicy': 'depthwise',\n",
        "    'lambda_': 1.0,\n",
        "    'subsample': 1.0,\n",
        "    'numRound': 1000,\n",
        "    'numWorkers': 1,\n",
        "    'verbosity': 1\n",
        "}\n",
        "classifier = XGBoostClassifier(**params).setLabelCol(label).setFeaturesCols(features)\n",
        "#YOU WILL GET AN ERROR EXECUTING THE ABOVE STATEMENT BECAUSE \n",
        "#XGBOOST does not support Spark 3.0.0. Please use Spark 2.4.3."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47gU9eWTzhy2"
      },
      "source": [
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gqj47DF5cdB",
        "outputId": "2b34bf17-cdec-444f-ee17-c583510e3469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covtype_test.parquet\t       spark-2.4.7-bin-hadoop2.7.tgz.1\n",
            "covtype_train.parquet\t       spark-2.4.7.tgz\n",
            "cudf-0.9.2-cuda10-1.jar        spark-2.4.7.tgz.1\n",
            "cudf-0.9.2-cuda10-1.jar.1      xgboost4j_2.x-1.0.0-Beta5.jar\n",
            "sample_data\t\t       xgboost4j_2.x-1.0.0-Beta5.jar.1\n",
            "spark-2.4.7-bin-hadoop2.7      xgboost4j-spark_2.x-1.0.0-Beta5.jar\n",
            "spark-2.4.7-bin-hadoop2.7.tgz  xgboost4j-spark_2.x-1.0.0-Beta5.jar.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2IB1xqq4Mvs"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/cudf-0.9.2-cuda10-1.jar,/content/xgboost4j_2.x-1.0.0-Beta5.jar,/content/xgboost4j-spark_2.x-1.0.0-Beta5.jar pyspark-shell'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Whoog50Uvy"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "#spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
        "#spark.conf.set(\"spark.driver.memory\", \"2g\")\n",
        "spark.sparkContext.addPyFile('/content/xgboost4j-spark_2.x-1.0.0-Beta5.jar')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUSXpcV94xdr"
      },
      "source": [
        "from ml.dmlc.xgboost4j.scala.spark import XGBoostClassificationModel, XGBoostClassifier\n",
        "from ml.dmlc.xgboost4j.scala.spark.rapids import GpuDataReader\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "covtyp = fetch_openml(name='covertype', version=4)\n",
        "\n",
        "cov_df = pd.DataFrame(data= np.c_[covtyp['data'], covtyp['target']],\n",
        "                     columns= covtyp['feature_names'] + ['target'])\n",
        "\n",
        "for cols in cov_df.columns:\n",
        "  cov_df[cols] = pd.to_numeric(cov_df[cols])\n",
        "\n",
        "cov_df['target'] = cov_df['target']-1\n",
        "train_df=cov_df.sample(frac=0.8,random_state=10) \n",
        "test_df=cov_df.drop(train_df.index)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR6cvKMK2Iat"
      },
      "source": [
        "train_data = GpuDataReader(spark).format('parquet').load('covtype_train.parquet')\n",
        "test_data = GpuDataReader(spark).format('parquet').load('covtype_test.parquet')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFiwZLNN2TJE"
      },
      "source": [
        "import pyarrow.parquet as pq\n",
        "pq_file=pq.read_table('covtype_train.parquet')\n",
        "label=\"target\"\n",
        "features = [ x for x in pq_file.column_names if x != label ]\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hc_QWHR1R-9"
      },
      "source": [
        "import time\n",
        "params = { \n",
        "    'eta': 0.1,\n",
        "    'gamma': 0.1,\n",
        "    'missing': 0.0,\n",
        "    'treeMethod': 'gpu_hist',\n",
        "    'maxDepth': 8, \n",
        "    'growPolicy': 'depthwise',\n",
        "    'lambda_': 1.0,\n",
        "    'subsample': 1.0,\n",
        "    'numRound': 1000,\n",
        "    'numWorkers': 1,\n",
        "    'verbosity': 1\n",
        "}\n",
        "classifier = XGBoostClassifier(**params).setLabelCol(label).setFeaturesCols(features)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJFLA4Nz2HIu"
      },
      "source": [
        "#Stil will get a problem. In you want to resolve - diconnect and connect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6UaltaO1f_k",
        "outputId": "5dfc193b-c94d-4835-fc84-a1bc479204f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model=classifier.fit(train_data)\n",
        "\n",
        "print(\"GPU Training Time: %s seconds\" % (str(time.time() - start_time)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Training Time: 27.310513496398926 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UNFJSJ51krM"
      },
      "source": [
        "!nvidia-smi\n",
        "model.write().overwrite().save('/content/model/')\n",
        "loaded_model = XGBoostClassificationModel().load('/content/model/')\n",
        "result=loaded_model.transform(test_data)\n",
        "result.show()\n",
        "MulticlassClassificationEvaluator().setLabelCol(label).evaluate(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkBAGXyy7cy5"
      },
      "source": [
        "!rm -rf /content/spark-3.0.1-bin-hadoop3.2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM3OgorX7ffM"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yKeRJo7hNb"
      },
      "source": [
        "!rm -rf /content/spark-3.0.1-bin-hadoop3.2.tgz.1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqtK_kOm7pH3"
      },
      "source": [
        "###Now it works maddy - you are a star"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}